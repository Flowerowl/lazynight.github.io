---
title: '非监督学习&#8211;聚类算法'
author: Flowerowl
layout: post
permalink: /2753.html
views:
  - 1444
duoshuo_thread_id:
  - 1220743779864322436
bot_views:
  - 3
categories:
  - 数据挖掘
tags:
  - 数据挖掘
  - 数据聚类
  - 算法
  - 统计
  - 非监督学习
---
俗话说：“物以类聚，人以群分”，在自然科学和社会科学中，存在着大量的分类问题。所谓类，通俗地说，就是指相似元素的集合。

 

为了进一步理解什么叫做聚类，请看一面的例子：

1.  地球人分三种，白种，黄种，黑种人，这是从肤色上分类的，这里的肤色是一种特征，一个人出现在你面前，他胸前没挂着自己是什么种人你也可以分别出来，也就是能自主分类。
2.  一个班的学生我们计算他的各科和的平均分，按平均分可以分为不及格，及格，良，优秀4个等级，这里的等级就是分类数目。这些分类结果也是由平均分这一本质特征来决定的，并不是说谁优秀谁就优秀的。
3.  试试想想在不知道中国老虎分为几类的情况下，你是怎么分类的。首先把所有老虎都抓起来慢慢研究，找出老虎特征间的本质不周再进行分类。而聚类就是能达到这种效果的方法。
4.  先看看下图：

 <img title="0_13083275648491.gif" src="http://lazynight.me/wp-content/uploads/2012/12/0_13083275648491.gif" alt="0 13083275648491" width="561" height="420" border="0" />

上图的点似乎围绕着3个中心点的，也就是3类的聚类中心。每个数据点只有2维坐标特征，并没有表明是属于哪一个类的特征。在分类问题中，如果要进行对已知分类的数据进行训练，这种方法叫监督学习。而聚类算法是在没有知道分类情况下的学习方式，这叫做非监督学习。

本文介绍一种k-mean聚类算法,这里的k就是分类的数目。

k-mean聚类算法如下：

1.  从数据点中，随机选取k个数据中心作为初始的聚类中心。例如k=3,则选择3个数据点
2.  分别计算每一个点到k个中心点的距离（本文计算的是欧式距离），如果当前计算的数据点离第i个（i=1,2,…,k）中心点最近，则把当前点归到第i类.
3.  重新计算k个聚类中心点。计算方式如下，如果第i类有n个数据点，则第i类新的中心为：

 <img title="0_1308327507RTY4.gif" src="http://lazynight.me/wp-content/uploads/2012/12/0_1308327507RTY4.gif" alt="0 1308327507RTY4" width="429" height="107" border="0" />

      4．如果新的聚类中心跟上一次的聚类中心比较变化小于某值算法结束，否则转到第二步。

聚类结果如下：

 <img title="0_13083274762qmR.gif" src="http://lazynight.me/wp-content/uploads/2012/12/0_13083274762qmR.gif" alt="0 13083274762qmR" width="560" height="420" border="0" />

转自：http://blog.csdn.net/huandaohack/article/details/6552807

转载请注明：[于哲的博客][1] &raquo; [非监督学习&#8211;聚类算法][2]

 [1]: http://localhost/wordpress
 [2]: http://localhost/wordpress/2753.html